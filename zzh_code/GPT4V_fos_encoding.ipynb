{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "def get_caption(frame, prompt, api_key):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt,\n",
    "                    },\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame[0]}\"}},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame[1]}\"}},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame[2]}\"}},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300,\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload, timeout=60)\n",
    "    caption = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    caption = caption.replace(\"\\n\", \" \")\n",
    "    return caption\n",
    "\n",
    "def get_video_length(cap):\n",
    "    return int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "def extract_frames(video_path, points=(0.2, 0.5, 0.8), base_64=False):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    length = get_video_length(cap)\n",
    "    points = [int(length * point) for point in points]\n",
    "    frames = []\n",
    "    if length < 3:\n",
    "        return frames, length\n",
    "    for point in points:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, point)\n",
    "        ret, frame = cap.read()\n",
    "        if not base_64:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "        else:\n",
    "            _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "            frame = base64.b64encode(buffer).decode(\"utf-8\")\n",
    "        frames.append(frame)\n",
    "    return frames, length\n",
    "\n",
    "\n",
    "def to_base64(image):\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"JPEG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "def gpt4v_inference(path_video, prompt):\n",
    "  # OpenAI API Key\n",
    "  api_key = \"*********\"\n",
    "\n",
    "  # Extract frames from video\n",
    "  frames, length = extract_frames(path_video, base_64=True)\n",
    "\n",
    "  headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "    \"model\": \"gpt-4-turbo\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": prompt\n",
    "          },\n",
    "          {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frames[0]}\"}},\n",
    "          {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frames[1]}\"}},\n",
    "          {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frames[2]}\"}}\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"max_tokens\": 300\n",
    "  }\n",
    "\n",
    "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "  predict = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "  predict = predict.replace(\"\\n\", \" \")\n",
    "  return predict, response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5, 11'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path_video = \"/home/artmed/Documents/autism_dataset/Data_10s_clips/20th_BAM_Hospital_Playtime_New Toys_1.mp4\"\n",
    "test_label = ['C+', 'O', 'Q+', 'S+', 'VI+']\n",
    "test_num_label = [3, 8, 10, 11, 13]\n",
    "\n",
    "\n",
    "prompt = '''\n",
    "A video is given by providing three frames in chronological order. \n",
    "Please choose one or more appropriate interaction styles or behaviors in the video.\n",
    "Please only reply with the numbers of the interaction styles or behaviors, separated by commas.\n",
    "The candidates of the interaction styles or behaviors are as follows:\n",
    "1. Appropriate verbal interactions\n",
    "2. Parent affection\n",
    "3. Positive contact\n",
    "4. Complaint\n",
    "5. Engaged activity of play\n",
    "6. Multiple instruction\n",
    "7. Non-compliance\n",
    "8. Oppositional\n",
    "9. Praise\n",
    "10. Positive question\n",
    "11. Positive social attention\n",
    "12. Positive specific instruction\n",
    "13. Positive vague instruction\n",
    "'''\n",
    "\n",
    "predict, response = gpt4v_inference(test_path_video, prompt = prompt)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "df_validation = pd.read_csv(r'validation.csv')\n",
    "df_FOS_used_label = pd.read_csv(r'FOS_used_label.csv')\n",
    "df_FOS_used_label['index'] += 1\n",
    "dict_used_label = df_FOS_used_label.set_index('mid').to_dict()['index']\n",
    "\n",
    "df_gpt4_predict_label = pd.DataFrame(columns=['path_video', 'one_hot_predict', 'one_hot_labels', 'raw_predict', 'labels'])\n",
    "for index, row in df_validation.iterrows():\n",
    "\n",
    "    # Get the path of the video at first\n",
    "    test_path_video = row['path']\n",
    "\n",
    "    # Get the labels of the video\n",
    "    test_labels = literal_eval(row['used_label'])\n",
    "    num_labels = [dict_used_label[label] for label in test_labels]\n",
    "    one_hot_labels = [1 if i in num_labels else 0 for i in range(1, 14)]\n",
    "\n",
    "    # Get the prediction from GPT-4\n",
    "    predict, response = gpt4v_inference(test_path_video, prompt = prompt)\n",
    "    predict = list(literal_eval(predict))\n",
    "    one_hot_predict = [1 if i in predict else 0 for i in range(1, 14)]\n",
    "\n",
    "    df_gpt4_predict_label.loc[index] = [test_path_video, one_hot_predict, one_hot_labels, predict, test_labels]\n",
    "    break\n",
    "# test_path_video = \"/home/artmed/Documents/autism_dataset/Data_10s_clips/20th_BAM_Hospital_Playtime_New Toys_1.mp4\"\n",
    "# test_label = ['C+', 'O', 'Q+', 'S+', 'VI+']\n",
    "# test_num_label = [3, 8, 10, 11, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_video</th>\n",
       "      <th>one_hot_predict</th>\n",
       "      <th>one_hot_labels</th>\n",
       "      <th>raw_predict</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/artmed/Documents/autism_dataset/Data_10s...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1]</td>\n",
       "      <td>[5, 11]</td>\n",
       "      <td>[C+, O, Q+, S+, VI+]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          path_video  \\\n",
       "0  /home/artmed/Documents/autism_dataset/Data_10s...   \n",
       "\n",
       "                           one_hot_predict  \\\n",
       "0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]   \n",
       "\n",
       "                            one_hot_labels raw_predict                labels  \n",
       "0  [0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1]     [5, 11]  [C+, O, Q+, S+, VI+]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt4_predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this video, a child, dressed in a vibrant orange jacket and blue bottoms, is navigating a play area in an indoor setting while a woman, likely a caregiver, is present in the background. The child progresses through the room, which is filled with colorful children's furniture and various toys including a yellow ball, a blue play mat, and small cars. As the child moves, the focus subtly shifts between the bustling activities and the surrounding child-friendly environment. The atmosphere is casual and educational, subtly highlighting the dynamic interaction between the youthful exuberance of the child and the calm oversight by the adult. The video captures the essence of a day at a child-centric environment, emphasizing learning and play in a safe and nurturing space.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A video is given by providing three frames in chronological order. Describe this video and its style to generate a description. Pay attention to all objects in the video. Do not describe each frame individually. Do not reply with words like 'first frame'. The description should be useful for AI to re-generate the video. The description should be less than six sentences. Here are some examples of good descriptions: 1. A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about. 2. Several giant wooly mammoths approach treading through a snowy meadow, their long wooly fur lightly blows in the wind as they walk, snow covered trees and dramatic snow capped mountains in the distance, mid afternoon light with wispy clouds and a sun high in the distance creates a warm glow, the low camera view is stunning capturing the large furry mammal with beautiful photography, depth of field. 3. Drone view of waves crashing against the rugged cliffs along Big Sur's garay point beach. The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the rocky shore. A small island with a lighthouse sits in the distance, and green shrubbery covers the cliff's edge. The steep drop from the road down to the beach is a dramatic feat, with the cliff’s edges jutting out over the sea. This is a view that captures the raw beauty of the coast and the rugged landscape of the Pacific Coast Highway.\"\n",
    "openAI_key = \"*********\"\n",
    "def get_caption(frame, prompt, api_key):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-turbo\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt,\n",
    "                    },\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame[0]}\"}},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame[1]}\"}},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{frame[2]}\"}},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300,\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()\n",
    "caption = get_caption(frame, prompt, openAI_key)\n",
    "caption = caption[\"choices\"][0][\"message\"][\"content\"]\n",
    "caption.replace(\"\\n\", \" \")\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A young child, dressed in an orange jacket and pink pants, is captured bustling around a vibrantly colored nursery room filled with toys and kid-sized furniture. Accompanied by an adult woman in a dark sweater and glasses, who seems engaged in a task, the video focuses on the child who explores various toys and activities casually scattered around the room. The setting is informal and cozy with soft ambient lighting, highlighting the sense of a safe and nurturing play environment. As the child moves energetically, the camera follows, blurring slightly due to the quick movement, creating a dynamic and lively atmosphere in the footage. The video captures the essence of childhood curiosity and the warmth of a caretaker’s supervision.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "import csv\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def get_video_length(cap):\n",
    "    return int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "def extract_frames(video_path, points=(0.2, 0.5, 0.8), base_64=False):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    length = get_video_length(cap)\n",
    "    points = [int(length * point) for point in points]\n",
    "    frames = []\n",
    "    if length < 3:\n",
    "        return frames, length\n",
    "    for point in points:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, point)\n",
    "        ret, frame = cap.read()\n",
    "        if not base_64:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "        else:\n",
    "            _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "            frame = base64.b64encode(buffer).decode(\"utf-8\")\n",
    "        frames.append(frame)\n",
    "    return frames, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_filelist(file_path):\n",
    "    Filelist = []\n",
    "    for home, dirs, files in os.walk(file_path):\n",
    "        for filename in files:\n",
    "            Filelist.append(os.path.join(home, filename))\n",
    "    return Filelist\n",
    "\n",
    "f = open(\"samples_training\", \"a\")\n",
    "paths_video = get_filelist('/home/artmed/PycharmProjects/dataset/UCF101')\n",
    "writer = csv.writer(f)\n",
    "for path_video in paths_video:\n",
    "    length = get_video_length(cv2.VideoCapture(path_video))\n",
    "    class_video = path_video.split('/UCF101/')[1].split('/')[0]\n",
    "    writer.writerow([path_video, class_video, length])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cav-mae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
