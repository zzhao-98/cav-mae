{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../src/\")\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use norm_pix_loss:  False\n",
      "Number of Audio Patches: 512, Visual Patches: 196\n",
      "Audio Positional Embedding Shape: torch.Size([1, 512, 768])\n",
      "Visual Positional Embedding Shape: torch.Size([1, 196, 768])\n",
      "wa 100 models from 1 to 100\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CAVMAEFT(\n",
       "    (patch_embed_a): PatchEmbed(\n",
       "      (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (patch_embed_v): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks_a): ModuleList(\n",
       "      (0-10): 11 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm1_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm1_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (blocks_v): ModuleList(\n",
       "      (0-10): 11 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm1_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm1_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (blocks_u): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm1_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm1_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp_head): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=768, out_features=13, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "audio_model_wa = models.CAVMAEFT(label_dim=13, modality_specific_depth=11)\n",
    "exp_dir = \"/home/artmed/PycharmProjects/cav-mae/zzh_code/exp/testmae01-full-cav-mae-ft-1e-5-5-0.95-1-bs128-ldaFalse-multimodal-fzFalse-h50-r3\"\n",
    "def wa_model(exp_dir, start_epoch, end_epoch):\n",
    "    sdA = torch.load(exp_dir + '/models/audio_model.' + str(start_epoch) + '.pth', map_location='cpu')\n",
    "    model_cnt = 1\n",
    "    for epoch in range(start_epoch+1, end_epoch+1):\n",
    "        sdB = torch.load(exp_dir + '/models/audio_model.' + str(epoch) + '.pth', map_location='cpu')\n",
    "        for key in sdA:\n",
    "            sdA[key] = sdA[key] + sdB[key]\n",
    "        model_cnt += 1\n",
    "    print('wa {:d} models from {:d} to {:d}'.format(model_cnt, start_epoch, end_epoch))\n",
    "    for key in sdA:\n",
    "        sdA[key] = sdA[key] / float(model_cnt)\n",
    "    return sdA\n",
    "\n",
    "audio_model_wa = torch.nn.DataParallel(audio_model_wa)\n",
    "sdA = wa_model(exp_dir, start_epoch=1, end_epoch=100)\n",
    "msg = audio_model_wa.load_state_dict(sdA, strict=True)\n",
    "print(msg)\n",
    "audio_model_wa.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use norm_pix_loss:  False\n",
      "Number of Audio Patches: 512, Visual Patches: 196\n",
      "Audio Positional Embedding Shape: torch.Size([1, 512, 768])\n",
      "Visual Positional Embedding Shape: torch.Size([1, 196, 768])\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CAVMAEFT(\n",
       "    (patch_embed_a): PatchEmbed(\n",
       "      (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (patch_embed_v): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks_a): ModuleList(\n",
       "      (0-10): 11 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm1_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm1_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (blocks_v): ModuleList(\n",
       "      (0-10): 11 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm1_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm1_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (blocks_u): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm1_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm1_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_a): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm_v): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (mlp_head): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=768, out_features=13, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "audio_model = models.CAVMAEFT(label_dim=13, modality_specific_depth=11)\n",
    "exp_dir = \"/home/artmed/PycharmProjects/cav-mae/zzh_code/exp/testmae01-full-cav-mae-ft-1e-5-5-0.95-1-bs128-ldaFalse-multimodal-fzFalse-h50-r3\"\n",
    "audio_model = torch.nn.DataParallel(audio_model)\n",
    "sdA = torch.load(exp_dir + '/models/best_audio_model.pth', map_location='cpu')\n",
    "msg = audio_model.load_state_dict(sdA, strict=True)\n",
    "print(msg)\n",
    "audio_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader as dataloader\n",
    "im_res = 224\n",
    "val_audio_conf = {'num_mel_bins': 128, 'target_length': 1024, 'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': \"audioset\",\n",
    "                  'mode':'eval', 'mean': -5.081, 'std': 4.4849, 'noise': False, 'im_res': im_res}\n",
    "\n",
    "data_val = \"/home/artmed/PycharmProjects/cav-mae/zzh_code/FOS_validation_dataset.json\"\n",
    "label_csv=\"/home/artmed/PycharmProjects/cav-mae/zzh_code/FOS_used_label.csv\"\n",
    "num_workers = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../src/\")\n",
    "from utilities import *\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from torch.cuda.amp import autocast,GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(audio_model, val_loader, loss_fn, output_pred=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_time = AverageMeter()\n",
    "    if not isinstance(audio_model, nn.DataParallel):\n",
    "        audio_model = nn.DataParallel(audio_model)\n",
    "    audio_model = audio_model.to(device)\n",
    "    audio_model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    A_predictions, A_targets, A_loss = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, (a_input, v_input, labels) in enumerate(val_loader):\n",
    "            a_input = a_input.to(device)\n",
    "            v_input = v_input.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                audio_output = audio_model(a_input, v_input, 'multimodal')\n",
    "\n",
    "            predictions = audio_output.to('cpu').detach()\n",
    "\n",
    "            A_predictions.append(predictions)\n",
    "            A_targets.append(labels)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "            loss = loss_fn(audio_output, labels)\n",
    "            A_loss.append(loss.to('cpu').detach())\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "        audio_output = torch.cat(A_predictions)\n",
    "        target = torch.cat(A_targets)\n",
    "        loss = np.mean(A_loss)\n",
    "\n",
    "        stats = calculate_stats(audio_output, target)\n",
    "\n",
    "    if output_pred == False:\n",
    "        return stats, loss\n",
    "    else:\n",
    "        # used for multi-frame evaluation (i.e., ensemble over frames), so return prediction and target\n",
    "        return stats, audio_output, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 1611 samples\n",
      "Using Label Smoothing: 0.0\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process audioset\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "not use noise augmentation\n",
      "number of classes is 13\n",
      "now in eval mode.\n",
      "now use frame 0 from total 10 frames\n",
      "now using 224 * 224 image input\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1611, 13])\n",
      "Here is the best!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "mAP of frame 0 is 0.5022\n",
      "Dataset has 1611 samples\n",
      "Using Label Smoothing: 0.0\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process audioset\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "not use noise augmentation\n",
      "number of classes is 13\n",
      "now in eval mode.\n",
      "now use frame 1 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "torch.Size([1611, 13])\n",
      "Here is the best!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "mAP of frame 1 is 0.5026\n",
      "Dataset has 1611 samples\n",
      "Using Label Smoothing: 0.0\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process audioset\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "not use noise augmentation\n",
      "number of classes is 13\n",
      "now in eval mode.\n",
      "now use frame 2 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "torch.Size([1611, 13])\n",
      "Here is the best!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "mAP of frame 2 is 0.5027\n",
      "Dataset has 1611 samples\n",
      "Using Label Smoothing: 0.0\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process audioset\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "not use noise augmentation\n",
      "number of classes is 13\n",
      "now in eval mode.\n",
      "now use frame 3 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "torch.Size([1611, 13])\n",
      "mAP of frame 3 is 0.5025\n",
      "Dataset has 1611 samples\n",
      "Using Label Smoothing: 0.0\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process audioset\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "not use noise augmentation\n",
      "number of classes is 13\n",
      "now in eval mode.\n",
      "now use frame 4 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "torch.Size([1611, 13])\n",
      "Here is the best!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "mAP of frame 4 is 0.5034\n",
      "Dataset has 1611 samples\n",
      "Using Label Smoothing: 0.0\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process audioset\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "not use noise augmentation\n",
      "number of classes is 13\n",
      "now in eval mode.\n",
      "now use frame 5 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "torch.Size([1611, 13])\n",
      "Here is the best!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "mAP of frame 5 is 0.5070\n",
      "Dataset has 1611 samples\n",
      "Using Label Smoothing: 0.0\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process audioset\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "not use noise augmentation\n",
      "number of classes is 13\n",
      "now in eval mode.\n",
      "now use frame 6 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "torch.Size([1611, 13])\n",
      "mAP of frame 6 is 0.5046\n",
      "Dataset has 1611 samples\n",
      "Using Label Smoothing: 0.0\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process audioset\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "not use noise augmentation\n",
      "number of classes is 13\n",
      "now in eval mode.\n",
      "now use frame 7 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "torch.Size([1611, 13])\n",
      "mAP of frame 7 is 0.5004\n",
      "Dataset has 1611 samples\n",
      "Using Label Smoothing: 0.0\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process audioset\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "not use noise augmentation\n",
      "number of classes is 13\n",
      "now in eval mode.\n",
      "now use frame 8 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "torch.Size([1611, 13])\n",
      "mAP of frame 8 is 0.5026\n",
      "Dataset has 1611 samples\n",
      "Using Label Smoothing: 0.0\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process audioset\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "not use noise augmentation\n",
      "number of classes is 13\n",
      "now in eval mode.\n",
      "now use frame 9 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "torch.Size([1611, 13])\n",
      "mAP of frame 9 is 0.5009\n"
     ]
    }
   ],
   "source": [
    "# Find the best mAP frame\n",
    "res = []\n",
    "multiframe_pred = []\n",
    "total_frames = 10\n",
    "best_mAP = 0\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "for frame in range(total_frames):\n",
    "    val_audio_conf['frame_use'] = frame\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataloader.AudiosetDataset(data_val, label_csv=label_csv, audio_conf=val_audio_conf),\n",
    "        batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    stats, audio_output, target = validate(audio_model, val_loader, loss_fn, output_pred=True)\n",
    "    print(audio_output.shape)\n",
    "\n",
    "    audio_output = torch.nn.functional.sigmoid(audio_output.float())\n",
    "\n",
    "    audio_output, target = audio_output.numpy(), target.numpy()\n",
    "    multiframe_pred.append(audio_output)\n",
    "    cur_res = np.mean([stat['AP'] for stat in stats])\n",
    "    if cur_res > best_mAP:\n",
    "        best_mAP = cur_res\n",
    "        print(\"Here is the best!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print('mAP of frame {:d} is {:.4f}'.format(frame, cur_res))\n",
    "\n",
    "    res.append(cur_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 1611 samples\n",
      "Using Label Smoothing: 0.0\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process audioset\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "not use noise augmentation\n",
      "number of classes is 13\n",
      "now in eval mode.\n",
      "now use frame 5 from total 10 frames\n",
      "now using 224 * 224 image input\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "val_audio_conf['frame_use'] = 5\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        dataloader.AudiosetDataset(data_val, label_csv=label_csv, audio_conf=val_audio_conf),\n",
    "        batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_time = AverageMeter()\n",
    "audio_model = audio_model.to(device)\n",
    "audio_model.eval()\n",
    "\n",
    "end = time.time()\n",
    "A_predictions, A_targets, A_loss = [], [], []\n",
    "with torch.no_grad():\n",
    "        for i, (a_input, v_input, labels) in enumerate(val_loader):\n",
    "                a_input = a_input.to(device)\n",
    "                v_input = v_input.to(device)\n",
    "\n",
    "                with autocast():\n",
    "                        audio_output = audio_model(a_input, v_input, 'multimodal')\n",
    "\n",
    "                predictions = audio_output.to('cpu').detach()\n",
    "\n",
    "                A_predictions.append(predictions)\n",
    "                A_targets.append(labels)\n",
    "\n",
    "                labels = labels.to(device)\n",
    "                loss = loss_fn(audio_output, labels)\n",
    "                A_loss.append(loss.to('cpu').detach())\n",
    "\n",
    "                batch_time.update(time.time() - end)\n",
    "                end = time.time()\n",
    "\n",
    "audio_output = torch.cat(A_predictions)\n",
    "target = torch.cat(A_targets)\n",
    "loss = np.mean(A_loss)\n",
    "\n",
    "stats = calculate_stats(audio_output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1611, 13])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_output.size()\n",
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1611])\n",
      "torch.Size([1611])\n",
      "torch.Size([1611])\n",
      "torch.Size([1611])\n",
      "torch.Size([1611])\n",
      "torch.Size([1611])\n",
      "torch.Size([1611])\n",
      "torch.Size([1611])\n",
      "torch.Size([1611])\n",
      "torch.Size([1611])\n",
      "torch.Size([1611])\n",
      "torch.Size([1611])\n",
      "torch.Size([1611])\n"
     ]
    }
   ],
   "source": [
    "# Calculate the confusion matrix\n",
    "threshold = 0.5\n",
    "for cls in range(audio_output.shape[1]):\n",
    "    predictions = audio_output[:, cls]\n",
    "    labels = target[:, cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0664,  0.6201,  1.2539,  ..., -2.4023, -3.0273, -2.4199],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cav-mae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
